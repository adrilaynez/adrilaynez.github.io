<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="dark">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Optimizing Matrix Multiplication in CUDA | Adrian Laynez</title>
<meta name="keywords" content="CUDA, C&#43;&#43;, Performance">
<meta name="description" content="The Bottleneck
Naive matrix multiplication is $O(N^3)$. In Python, torch.matmul hides this complexity, but in C&#43;&#43;, we face the raw memory latency.
The Code
Here is the naive implementation vs the tiled kernel:


1
2
3
4
5
6


// Naive implementation
__global__ void matmul_naive(float* A, float* B, float* C, int N) {
    int row = blockIdx.y * blockDim.y &#43; threadIdx.y;
    int col = blockIdx.x * blockDim.x &#43; threadIdx.x;
    // ...
}


Performance Analysis
![Benchmark Graph](https://quickchart.io/chart?c={type:&#39;bar&#39;,data:{labels:[&#39;CPU&#39;,&#39;Naive GPU&rsquo;,&lsquo;Tiled GPU&rsquo;],datasets:[{label:&lsquo;Time (ms)&rsquo;,data:[450,45,3]}]}})
As shown above, the Tiled approach is 15x faster.">
<meta name="author" content="Adrian Laynez Ortiz">
<link rel="canonical" href="https://adrianlaynez.dev/posts/optimization-matrix/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.e9277b006c5b6c7c3cb0bea9bbe6f1e1fc7031e938bbdeaab7bc2cb0a29b365c.css" integrity="sha256-6Sd7AGxbbHw8sL6pu&#43;bx4fxwMek4u96qt7wssKKbNlw=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://adrianlaynez.dev/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://adrianlaynez.dev/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://adrianlaynez.dev/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://adrianlaynez.dev/apple-touch-icon.png">
<link rel="mask-icon" href="https://adrianlaynez.dev/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://adrianlaynez.dev/posts/optimization-matrix/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "light") {
        document.querySelector("html").dataset.theme = 'light';
    }

</script><meta property="og:url" content="https://adrianlaynez.dev/posts/optimization-matrix/">
  <meta property="og:site_name" content="Adrian Laynez">
  <meta property="og:title" content="Optimizing Matrix Multiplication in CUDA">
  <meta property="og:description" content="The Bottleneck Naive matrix multiplication is $O(N^3)$. In Python, torch.matmul hides this complexity, but in C&#43;&#43;, we face the raw memory latency.
The Code Here is the naive implementation vs the tiled kernel:
1 2 3 4 5 6 // Naive implementation __global__ void matmul_naive(float* A, float* B, float* C, int N) { int row = blockIdx.y * blockDim.y &#43; threadIdx.y; int col = blockIdx.x * blockDim.x &#43; threadIdx.x; // ... } Performance Analysis ![Benchmark Graph](https://quickchart.io/chart?c={type:&#39;bar&#39;,data:{labels:[&#39;CPU&#39;,&#39;Naive GPU’,‘Tiled GPU’],datasets:[{label:‘Time (ms)’,data:[450,45,3]}]}}) As shown above, the Tiled approach is 15x faster.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2026-02-01T00:00:00+00:00">
    <meta property="article:modified_time" content="2026-02-01T00:00:00+00:00">
    <meta property="article:tag" content="CUDA">
    <meta property="article:tag" content="C&#43;&#43;">
    <meta property="article:tag" content="Performance">
    <meta property="og:image" content="https://adrianlaynez.dev/posts/optimization-matrix/matrix.png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://adrianlaynez.dev/posts/optimization-matrix/matrix.png">
<meta name="twitter:title" content="Optimizing Matrix Multiplication in CUDA">
<meta name="twitter:description" content="The Bottleneck
Naive matrix multiplication is $O(N^3)$. In Python, torch.matmul hides this complexity, but in C&#43;&#43;, we face the raw memory latency.
The Code
Here is the naive implementation vs the tiled kernel:


1
2
3
4
5
6


// Naive implementation
__global__ void matmul_naive(float* A, float* B, float* C, int N) {
    int row = blockIdx.y * blockDim.y &#43; threadIdx.y;
    int col = blockIdx.x * blockDim.x &#43; threadIdx.x;
    // ...
}


Performance Analysis
![Benchmark Graph](https://quickchart.io/chart?c={type:&#39;bar&#39;,data:{labels:[&#39;CPU&#39;,&#39;Naive GPU&rsquo;,&lsquo;Tiled GPU&rsquo;],datasets:[{label:&lsquo;Time (ms)&rsquo;,data:[450,45,3]}]}})
As shown above, the Tiled approach is 15x faster.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://adrianlaynez.dev/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Optimizing Matrix Multiplication in CUDA",
      "item": "https://adrianlaynez.dev/posts/optimization-matrix/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Optimizing Matrix Multiplication in CUDA",
  "name": "Optimizing Matrix Multiplication in CUDA",
  "description": "The Bottleneck Naive matrix multiplication is $O(N^3)$. In Python, torch.matmul hides this complexity, but in C++, we face the raw memory latency.\nThe Code Here is the naive implementation vs the tiled kernel:\n1 2 3 4 5 6 // Naive implementation __global__ void matmul_naive(float* A, float* B, float* C, int N) { int row = blockIdx.y * blockDim.y + threadIdx.y; int col = blockIdx.x * blockDim.x + threadIdx.x; // ... } Performance Analysis ![Benchmark Graph](https://quickchart.io/chart?c={type:'bar',data:{labels:['CPU','Naive GPU\u0026rsquo;,\u0026lsquo;Tiled GPU\u0026rsquo;],datasets:[{label:\u0026lsquo;Time (ms)\u0026rsquo;,data:[450,45,3]}]}}) As shown above, the Tiled approach is 15x faster.\n",
  "keywords": [
    "CUDA", "C++", "Performance"
  ],
  "articleBody": "The Bottleneck Naive matrix multiplication is $O(N^3)$. In Python, torch.matmul hides this complexity, but in C++, we face the raw memory latency.\nThe Code Here is the naive implementation vs the tiled kernel:\n1 2 3 4 5 6 // Naive implementation __global__ void matmul_naive(float* A, float* B, float* C, int N) { int row = blockIdx.y * blockDim.y + threadIdx.y; int col = blockIdx.x * blockDim.x + threadIdx.x; // ... } Performance Analysis ![Benchmark Graph](https://quickchart.io/chart?c={type:'bar',data:{labels:['CPU','Naive GPU’,‘Tiled GPU’],datasets:[{label:‘Time (ms)’,data:[450,45,3]}]}}) As shown above, the Tiled approach is 15x faster.\n",
  "wordCount" : "88",
  "inLanguage": "en",
  "image":"https://adrianlaynez.dev/posts/optimization-matrix/matrix.png","datePublished": "2026-02-01T00:00:00Z",
  "dateModified": "2026-02-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Adrian Laynez Ortiz"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://adrianlaynez.dev/posts/optimization-matrix/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Adrian Laynez",
    "logo": {
      "@type": "ImageObject",
      "url": "https://adrianlaynez.dev/favicon.ico"
    }
  }
}
</script>
</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://adrianlaynez.dev/" accesskey="h" title="Adrian Laynez (Alt + H)">Adrian Laynez</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://adrianlaynez.dev/posts/" title="Blog">
                    <span>Blog</span>
                </a>
            </li>
            <li>
                <a href="https://adrianlaynez.dev/projects/" title="Projects">
                    <span>Projects</span>
                </a>
            </li>
            <li>
                <a href="https://adrianlaynez.dev/about-me/" title="About">
                    <span>About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Optimizing Matrix Multiplication in CUDA
    </h1>
    <div class="post-meta"><span title='2026-02-01 00:00:00 +0000 UTC'>February 1, 2026</span>&nbsp;·&nbsp;<span>Adrian Laynez Ortiz</span>

</div>
  </header> 
<figure class="entry-cover">
            <img loading="eager"
                srcset='https://adrianlaynez.dev/posts/optimization-matrix/matrix_hu_8fcf0f22517e9881.png 360w,https://adrianlaynez.dev/posts/optimization-matrix/matrix_hu_b3157b11e8c394aa.png 480w,https://adrianlaynez.dev/posts/optimization-matrix/matrix_hu_72eb1a96bac0644a.png 720w,https://adrianlaynez.dev/posts/optimization-matrix/matrix.png 980w'
                src="https://adrianlaynez.dev/posts/optimization-matrix/matrix.png"
                sizes="(min-width: 768px) 720px, 100vw"
                width="980" height="980"
                alt="Matrix Multiplication Tiling Visualization">
        <figcaption>Tiling strategy to reduce global memory access.</figcaption>
</figure>
  <div class="post-content"><h2 id="the-bottleneck">The Bottleneck<a hidden class="anchor" aria-hidden="true" href="#the-bottleneck">#</a></h2>
<p>Naive matrix multiplication is $O(N^3)$. In Python, <code>torch.matmul</code> hides this complexity, but in C++, we face the raw memory latency.</p>
<h3 id="the-code">The Code<a hidden class="anchor" aria-hidden="true" href="#the-code">#</a></h3>
<p>Here is the naive implementation vs the tiled kernel:</p>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span><span style="color:#6272a4">// Naive implementation
</span></span></span><span style="display:flex;"><span>__global__ <span style="color:#8be9fd">void</span> <span style="color:#50fa7b">matmul_naive</span>(<span style="color:#8be9fd">float</span><span style="color:#ff79c6">*</span> A, <span style="color:#8be9fd">float</span><span style="color:#ff79c6">*</span> B, <span style="color:#8be9fd">float</span><span style="color:#ff79c6">*</span> C, <span style="color:#8be9fd">int</span> N) {
</span></span><span style="display:flex;"><span>    <span style="color:#8be9fd">int</span> row <span style="color:#ff79c6">=</span> blockIdx.y <span style="color:#ff79c6">*</span> blockDim.y <span style="color:#ff79c6">+</span> threadIdx.y;
</span></span><span style="display:flex;"><span>    <span style="color:#8be9fd">int</span> col <span style="color:#ff79c6">=</span> blockIdx.x <span style="color:#ff79c6">*</span> blockDim.x <span style="color:#ff79c6">+</span> threadIdx.x;
</span></span><span style="display:flex;"><span>    <span style="color:#6272a4">// ...
</span></span></span><span style="display:flex;"><span>}</span></span></code></pre></td></tr></table>
</div>
</div>
<h3 id="performance-analysis">Performance Analysis<a hidden class="anchor" aria-hidden="true" href="#performance-analysis">#</a></h3>
<p>![Benchmark Graph](<a href="https://quickchart.io/chart?c=%7Btype:'bar',data:%7Blabels:%5B'CPU','Naive">https://quickchart.io/chart?c={type:'bar',data:{labels:['CPU','Naive</a> GPU&rsquo;,&lsquo;Tiled GPU&rsquo;],datasets:[{label:&lsquo;Time (ms)&rsquo;,data:[450,45,3]}]}})
<em>As shown above, the Tiled approach is 15x faster.</em></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://adrianlaynez.dev/tags/cuda/">CUDA</a></li>
      <li><a href="https://adrianlaynez.dev/tags/c&#43;&#43;/">C&#43;&#43;</a></li>
      <li><a href="https://adrianlaynez.dev/tags/performance/">Performance</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2026 <a href="https://adrianlaynez.dev/">Adrian Laynez</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
